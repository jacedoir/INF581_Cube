{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from typing import List, Callable\n",
    "import collections\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sources:\n",
    "# - https://gymnasium.farama.org/api/env/\n",
    "# - https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#sphx-glr-tutorials-gymnasium-basics-environment-creation-py\n",
    "\n",
    "class CubeEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\", \"rgb_array\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        self.size = 2 # size of rubik's cube\n",
    "        self.state = np.chararray((6, self.size, self.size), unicode=True) # initialize cube config\n",
    "\n",
    "        # a chaque fois qu'on rajoute une dim, on rajoute 6 coups possible (2 par nouveau plan, x3 car nb d'axes de l'espace)\n",
    "        self.action_space = spaces.Discrete(2 * 3 * (self.size - 1))\n",
    "\n",
    "        # render mode\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self._position_dict = {0: \"left\", 1:\"top\", 2:\"front\", 3:\"bottom\", 4:\"back\", 5:\"right\"}\n",
    "        self._color_dict = {\"r\": \"red\", \"g\": \"green\", \"b\": \"blue\", \"o\": \"orange\", \"y\": \"yellow\", \"w\": \"black\", \"\":\"none\"}\n",
    "        self._action_map = { # !!!!!! WARNING !!!!!! CHANGE WITH SIZE\n",
    "            0: lambda: self._horizontale_rotation(0, 1),\n",
    "            1: lambda: self._horizontale_rotation(0, -1),\n",
    "            2: lambda: self._verticale_rotation(0, 1),\n",
    "            3: lambda: self._verticale_rotation(0, -1),\n",
    "            4: lambda: self._face_rotation(0, 1),\n",
    "            5: lambda: self._face_rotation(0, -1)\n",
    "        }\n",
    "\n",
    "        self.window_size = 512  # The size of the PyGame window\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self.state\n",
    "    \n",
    "    def _get_info(self):\n",
    "        \"\"\"Fully known environment\"\"\"\n",
    "        return {}\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Puts Rubik's cube back to fully solved\"\"\"\n",
    "        self.state[0] = np.array([[\"o\"]*self.size]*self.size)\n",
    "        self.state[1] = np.array([[\"w\"]*self.size]*self.size)\n",
    "        self.state[2] = np.array([[\"g\"]*self.size]*self.size)\n",
    "        self.state[3] = np.array([[\"y\"]*self.size]*self.size)\n",
    "        self.state[4] = np.array([[\"r\"]*self.size]*self.size)\n",
    "        self.state[5] = np.array([[\"b\"]*self.size]*self.size)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        # rotate cube according to action map\n",
    "        self._action_map[action]()\n",
    "\n",
    "        # get info and everything\n",
    "        terminated = self._is_solved(self.state)\n",
    "        reward = self.reward(self.state)\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        # render if necessary\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "    \n",
    "    def reward(self, state):\n",
    "        # TODO\n",
    "        return int(self._is_solved(state))\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"Renders one frame\"\"\"\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "    \n",
    "    def _render_frame(self):\n",
    "\n",
    "        # init window\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # First we draw the target\n",
    "        pygame.draw.rect(\n",
    "            canvas,\n",
    "            (255, 0, 0),\n",
    "            pygame.Rect(\n",
    "                pix_square_size * self._target_location,\n",
    "                (pix_square_size, pix_square_size),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def _is_solved(self, state):\n",
    "        for i in range(6):\n",
    "            if len(np.unique(state[i])) != 1: # check for all face uniqueness\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _horizontale_rotation(self, row, direction):\n",
    "        new_state = np.copy(self.state)\n",
    "        if row > self.size-1:\n",
    "            raise ValueError(\"Row number must be between 0 and \", self.size-1)\n",
    "        if direction == 1: #To the right\n",
    "            new_state[2, row, :] = self.state[0, row, :]\n",
    "            new_state[5, row, :] = self.state[2, row, :]\n",
    "            new_state[4, row, :] = self.state[5, row, :]\n",
    "            new_state[0, row, :] = self.state[4, row, :]\n",
    "            if row == 0:\n",
    "                new_state[1] = np.rot90(self.state[1])\n",
    "            elif row == self.size-1:\n",
    "                new_state[3] = np.rot90(self.state[3],3)\n",
    "        elif direction == -1: #To the left\n",
    "            new_state[0, row, :] = self.state[2, row, :]\n",
    "            new_state[2, row, :] = self.state[5, row, :]\n",
    "            new_state[5, row, :] = self.state[4, row, :]\n",
    "            new_state[4, row, :] = self.state[0, row, :]\n",
    "            if row == 0:\n",
    "                new_state[1] = np.rot90(self.state[1],3)\n",
    "            elif row == self.size-1:\n",
    "                new_state[3] = np.rot90(self.state[3])\n",
    "        self.state = new_state\n",
    "\n",
    "    def _verticale_rotation(self, column, direction):\n",
    "        new_state = np.copy(self.state)\n",
    "        if column > self.size-1:\n",
    "            raise ValueError(\"Column number must be between 0 and \", self.size-1)\n",
    "        if direction == 1: #to the top\n",
    "            new_state[2, :, column] = self.state[3, :, column]\n",
    "            new_state[3, :, column] = self.state[4, :, column]\n",
    "            new_state[4, :, column] = self.state[1, :, column]\n",
    "            new_state[1, :, column] = self.state[2, :, column]\n",
    "            if column == 0:\n",
    "                new_state[0] = np.rot90(self.state[0])\n",
    "            elif column == self.size-1:\n",
    "                new_state[5] = np.rot90(self.state[5],3)\n",
    "        elif direction == -1: #to the bottom\n",
    "            new_state[3, :, column] = self.state[2, :, column]\n",
    "            new_state[4, :, column] = self.state[3, :, column]\n",
    "            new_state[1, :, column] = self.state[4, :, column]\n",
    "            new_state[2, :, column] = self.state[1, :, column]\n",
    "            if column == 0:\n",
    "                new_state[0] = np.rot90(self.state[0],3)\n",
    "            elif column == self.size-1:\n",
    "                new_state[5] = np.rot90(self.state[5])\n",
    "        self.state = new_state\n",
    "    \n",
    "    def _face_rotation(self, face, direction):\n",
    "        new_state = np.copy(self.state)\n",
    "        if face > self.size-1:\n",
    "            raise ValueError(\"Face number must be between 0 and \", self.size-1)\n",
    "        if direction == 1: #clockwise\n",
    "            new_state[1,self.size-1-face,:] = self.state[0,:,self.size-1-face]\n",
    "            new_state[5, :, face] = self.state[1,self.size-1-face,:]\n",
    "            new_state[3,face,:] = self.state[5,:,self.size-1-face]\n",
    "            new_state[0,:,self.size-1-face] = self.state[3,face,:]\n",
    "            if face == 0:\n",
    "                new_state[2] = np.rot90(self.state[2],3)\n",
    "            elif face == self.size-1:\n",
    "                new_state[4] = np.rot90(self.state[4])\n",
    "        elif direction == -1: #counterclockwise\n",
    "            new_state[0,:,self.size-1-face] = self.state[1,self.size-1-face,:]\n",
    "            new_state[1,self.size-1-face,:] = self.state[5, :, self.size-1-face]\n",
    "            new_state[5, :, face] = self.state[3,face,:]\n",
    "            new_state[3,face,:] = self.state[0,:,self.size-1-face]\n",
    "            if face == 0:\n",
    "                new_state[2] = np.rot90(self.state[2])\n",
    "            elif face == self.size-1:\n",
    "                new_state[4] = np.rot90(self.state[4],3)\n",
    "        self.state = new_state\n",
    "\n",
    "    def shuffle(self, n_moves):\n",
    "        # TODO (not important) able to use seed\n",
    "        for i in range(n_moves):\n",
    "            face = np.random.randint(self.size)\n",
    "            direction = np.random.choice([-1,1])\n",
    "            operation = np.random.randint(3)\n",
    "            if operation == 0:\n",
    "                self.horizontale_rotation(face, direction)\n",
    "            elif operation == 1:\n",
    "                self.verticale_rotation(face, direction)\n",
    "            else:\n",
    "                self.face_rotation(face, direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CubeEnv' object has no attribute 'print_2D'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[60], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m CubeEnv()\n\u001b[1;32m----> 2\u001b[0m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_2D\u001b[49m()\n\u001b[0;32m      3\u001b[0m env\u001b[38;5;241m.\u001b[39mshuffle(\u001b[38;5;241m25\u001b[39m)\n\u001b[0;32m      4\u001b[0m env\u001b[38;5;241m.\u001b[39mprint_2D()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CubeEnv' object has no attribute 'print_2D'"
     ]
    }
   ],
   "source": [
    "env = CubeEnv()\n",
    "env.print_2D()\n",
    "env.shuffle(25)\n",
    "env.print_2D()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modèle utilisé comme contrôleur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1006862838.py, line 29)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[41], line 29\u001b[1;36m\u001b[0m\n\u001b[1;33m    weights =\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Source: TD6 INF581\n",
    "\n",
    "n_input  = 6 * 6 * CUBE_SIZE * CUBE_SIZE # 6 = nombre de faces // size^2 = nombre de couleurs par face // 6 = one-hot encoding des couleurs\n",
    "n_output = env.action_space.size()\n",
    "n_dense  = 256 # au pif, un pax avait choisi 1024 pour le 3x3\n",
    "\n",
    "# Setup cuda/cpu device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Set the device to CUDA if available, otherwise use CPU\n",
    "print(f\"PyTorch will train neural networks on {device}\")\n",
    "\n",
    "class QNetwork(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QNetwork, self).__init__()\n",
    "        \n",
    "        self.l1 = nn.Linear(n_input, n_dense)\n",
    "        self.l2 = nn.Linear(n_dense, n_dense)\n",
    "        self.l3 = nn.Linear(n_dense, n_dense)\n",
    "        self.l4 = nn.Linear(n_dense, n_output)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.l1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.l4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \n",
    "    def __init__(self,\n",
    "                 epsilon_start: float,\n",
    "                 epsilon_min: float,\n",
    "                 epsilon_decay: float,\n",
    "                 q_network: torch.nn.Module):\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_network = q_network\n",
    "\n",
    "    def __call__(self, state: np.ndarray) -> np.int64:\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        \n",
    "        state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = self.q_network(state_tensor)\n",
    "        \n",
    "        return torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy=EpsilonGreedy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "rect = patches.Rectangle((0, 0), width=.5, height=.5, color='aqua')\n",
    "ax.add_patch(rect)\n",
    "fig.canvas.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinimumExponentialLR(torch.optim.lr_scheduler.ExponentialLR):\n",
    "    \n",
    "    def __init__(self, optimizer: torch.optim.Optimizer, lr_decay: float, last_epoch: int = -1, min_lr: float = 1e-6):\n",
    "        self.min_lr = min_lr\n",
    "        super().__init__(optimizer, lr_decay, last_epoch=-1)\n",
    "\n",
    "    def get_lr(self):\n",
    "        return [\n",
    "            max(base_lr * self.gamma ** self.last_epoch, self.min_lr)\n",
    "            for base_lr in self.base_lrs\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity: int):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]:\n",
    "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
    "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn2_agent(env: CubeEnv,\n",
    "                     q_network: torch.nn.Module,\n",
    "                     target_q_network: torch.nn.Module,\n",
    "                     optimizer: torch.optim.Optimizer,\n",
    "                     loss_fn: Callable,\n",
    "                     epsilon_greedy: EpsilonGreedy,\n",
    "                     device: torch.device,\n",
    "                     lr_scheduler: _LRScheduler,\n",
    "                     num_episodes: int,\n",
    "                     gamma: float,\n",
    "                     batch_size: int,\n",
    "                     replay_buffer: ReplayBuffer,\n",
    "                     target_q_network_sync_period: int) -> List[float]:\n",
    "    iteration = 0\n",
    "    episode_reward_list = []\n",
    "\n",
    "    for episode_index in tqdm(range(1, num_episodes)):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "\n",
    "        while True:\n",
    "\n",
    "            # Get action, next_state and reward\n",
    "            action = epsilon_greedy(state)\n",
    "\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Update the q_network weights with a batch of experiences from the buffer\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
    "\n",
    "                # Convert to PyTorch tensors\n",
    "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
    "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
    "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
    "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
    "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
    "\n",
    "                target_tensor = batch_rewards_tensor + gamma * target_q_network(batch_next_states_tensor).max(dim=1)[0] * (1 - batch_dones_tensor)\n",
    "                output_tensor = q_network(batch_states_tensor).index_select(dim=1, index=batch_actions_tensor)\n",
    "                loss = loss_fn(output_tensor, target_tensor)\n",
    "\n",
    "                # Optimize the model\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                lr_scheduler.step()\n",
    "\n",
    "            # Update the target q-network\n",
    "\n",
    "            # Every few training steps (e.g., every 100 steps), the weights of the target network are updated with the weights of the Q-network\n",
    "            if iteration % target_q_network_sync_period == 0:\n",
    "                target_q_network.load_state_dict(q_network.state_dict())\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        episode_reward_list.append(episode_reward)\n",
    "        epsilon_greedy.decay_epsilon()\n",
    "\n",
    "    return episode_reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUMBER_OF_TRAININGS = 20\n",
    "dqn2_trains_result_list = [[], [], []]\n",
    "\n",
    "for train_index in range(NUMBER_OF_TRAININGS):\n",
    "    q_network = QNetwork().to(device) # q network\n",
    "    target_q_network = QNetwork().to(device) # target q network\n",
    "    target_q_network.load_state_dict(q_network.state_dict()) # load same weights\n",
    "\n",
    "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
    "    #lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
    "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "    epsilon_greedy = EpsilonGreedy(epsilon_start=0.82, epsilon_min=0.013, epsilon_decay=0.9675, q_network=q_network)\n",
    "\n",
    "    replay_buffer = ReplayBuffer(2000)\n",
    "\n",
    "    # Train the q-network\n",
    "\n",
    "    episode_reward_list = train_dqn2_agent(q_network,\n",
    "                                           target_q_network,\n",
    "                                           optimizer,\n",
    "                                           loss_fn,\n",
    "                                           epsilon_greedy,\n",
    "                                           device,\n",
    "                                           lr_scheduler,\n",
    "                                           num_episodes=150,\n",
    "                                           gamma=0.9,\n",
    "                                           batch_size=128,\n",
    "                                           replay_buffer=replay_buffer,\n",
    "                                           target_q_network_sync_period=30)\n",
    "    dqn2_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
    "    dqn2_trains_result_list[1].extend(episode_reward_list)\n",
    "    dqn2_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
    "\n",
    "dqn2_trains_result_df = pd.DataFrame(np.array(dqn2_trains_result_list).T, columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"])\n",
    "dqn2_trains_result_df[\"agent\"] = \"DQN 2015\"\n",
    "\n",
    "# Save the action-value estimation function\n",
    "torch.save(q_network, \"dqn2_q_network.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
